* You chose **maximum SRE mode**.
* Letâ€™s do **all three for `nginx`**, clean and reusable:

* 1ï¸âƒ£ **Simulate breaking nginx on purpose**
* 2ï¸âƒ£ **Troubleshoot it step-by-step**
* 3ï¸âƒ£ **End with a reusable on-call runbook template**

You can literally reuse this in real incidents or interviews.

---

# ğŸ”¥ NGINX FAILURE DRILL (SIMULATED)

## Target service

* **Service:** nginx
* **Role:** Web server / reverse proxy
* **Impact:** User-facing traffic

---

## ğŸš¨ Pager Alert

* > Website returning 502 errors
* > Requests timing out intermittently
* > CPU usage spiking

---

## Step 1 â€” Health snapshot (muscle memory)

```bash
systemctl status nginx
```

**Output**

```
nginx.service - A high performance web server
Active: active (running)
```

ğŸ§  *Service is â€œupâ€ but may be unhealthy.*

---

```bash
top
```

```
PID   USER   %CPU  %MEM  COMMAND
3112  www    165.2  1.4   nginx
```

ğŸ§  *Worker process burning CPU.*

---

```bash
df -h
```

```
/dev/sda1   40G   38G   2G  95% /
```

ğŸ§  *Disk pressure approaching critical.*

---

## Step 2 â€” Network validation

```bash
ss -tulpn | grep nginx
```

```
LISTEN 0 511 0.0.0.0:80 users:(("nginx",pid=3112))
```

ğŸ§  *Port is listening.*

---

```bash
curl -I http://localhost
```

```
HTTP/1.1 502 Bad Gateway
```

ğŸ§  *Frontend up, backend failing.*

---

## Step 3 â€” Logs (where truth lives)

```bash
tail -n 50 /var/log/nginx/error.log
```

```
connect() failed (111: Connection refused) while connecting to upstream
```

ğŸ§  *Upstream app is dead or unreachable.*

---

```bash
du -sh /var/log/nginx
```

```
14G /var/log/nginx
```

ğŸ§  *Log spam explains disk pressure.*

---

## Step 4 â€” Root cause found ğŸ˜ˆ

Someone **accidentally enabled debug logging**:

```nginx
error_log /var/log/nginx/error.log debug;
```

This causes:

* Massive log volume
* Disk pressure
* CPU spike
* 502s under load

---

## Step 5 â€” Containment & fix

```bash
sed -i 's/debug;/warn;/' /etc/nginx/nginx.conf
```

```bash
nginx -t
systemctl reload nginx
```

```bash
truncate -s 0 /var/log/nginx/error.log
```

âœ… CPU drops
âœ… Disk stabilizes
âœ… 502s disappear

---

# ğŸ§  QUICK FINDINGS

* nginx process healthy but overloaded
* Upstream errors amplified by debug logging
* Disk nearly full due to runaway logs
* Reload (not restart) sufficient

---

# ğŸ§¯ IF THIS WORSENS (NEXT STEPS)

1. Temporarily disable access logs

   ```bash
   access_log off;
   ```

2. Rate-limit requests

   ```nginx
   limit_req zone=one burst=10 nodelay;
   ```

3. Capture traffic

   ```bash
   tcpdump -i eth0 port 80
   ```

---

# ğŸ“˜ REUSABLE ON-CALL RUNBOOK TEMPLATE 

```
#NGINX On-Call Runbook

##Service Overview
- Service name:
- Ports:
- Upstream dependencies:

## Immediate Checks (Run in Order)
1. systemctl status nginx
2. top / ps --sort=-%cpu
3. df -h
4. ss -tulpn | grep nginx
5. curl -I localhost

## Logs
- tail -n 50 /var/log/nginx/error.log
- du -sh /var/log/nginx

## Common Failure Patterns
- 502 errors â†’ upstream down
- High CPU â†’ log level, request flood
- Disk full â†’ access/error logs

## Safe Actions
- nginx -t
- systemctl reload nginx
- truncate logs (never rm)

## Escalation Triggers
- Reload fails
- Disk refills in <10 minutes
- Errors persist after upstream restart

## Notes / Learnings
- (fill during incident)
```

---

## ğŸ¯ What you just practiced

* âœ” Real nginx failure
* âœ” CPU + disk + logs correlation
* âœ” Safe reload vs restart
* âœ” Interview-grade runbook
* âœ” True on-call thinking

---


