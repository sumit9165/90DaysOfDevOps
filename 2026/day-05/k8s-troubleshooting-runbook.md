
# üí£ KUBERNETES POD CRASHLOOP + OOM KILL

## Target service

**Service:** `web-app` deployment in Kubernetes
**Impact:** Pod crashlooping, high memory usage

---

## üö® Pager Alert

> Deployment has pods in `CrashLoopBackOff`
> Users report intermittent 500s

---

## Step 1 ‚Äî Inspect pods

```bash
kubectl get pods -n production
```

```
NAME                     READY   STATUS             RESTARTS   AGE
web-app-6d7f89f6b7-hj9k2 0/1     CrashLoopBackOff   5          12m
```

---

## Step 2 ‚Äî Check pod logs

```bash
kubectl logs web-app-6d7f89f6b7-hj9k2 -n production
```

```
RuntimeError: Memory allocation failed
```

**Observed:** Pod OOMing ‚Üí crashloop

---

## Step 3 ‚Äî Describe pod for Kubernetes events

```bash
kubectl describe pod web-app-6d7f89f6b7-hj9k2 -n production
```

```
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Warning  OOMKilled    5m    kubelet            Container killed due to memory usage
```

**Root cause:** Container exceeded memory limit

---

## Step 4 ‚Äî Containment

* Temporarily scale deployment to reduce pressure:

```bash
kubectl scale deployment web-app --replicas=0 -n production
```

* Adjust memory limits and requests in the manifest:

```yaml
resources:
  requests:
    memory: "512Mi"
  limits:
    memory: "1024Mi"
```

* Redeploy:

```bash
kubectl apply -f web-app-deployment.yaml
kubectl scale deployment web-app --replicas=3 -n production
```

‚úÖ Pod stabilizes, no CrashLoopBackOff

---

## If this worsens (next steps)

1. Enable heap dumps for debugging
2. Use `kubectl exec` to inspect memory usage inside container
3. Add horizontal pod autoscaling to handle spikes

---

# üß† CACHING FAILURE (Redis meltdown)

## Pager Alert

> Cache latency spikes ‚Üí DB load increases
> App response slows, timeouts occur

---

## Step 1 ‚Äî Check Redis status

```bash
redis-cli ping
```

```
PONG
```

‚úÖ Redis alive, but may be overloaded

---

## Step 2 ‚Äî Inspect memory

```bash
redis-cli info memory
```

```
used_memory: 1050000000
maxmemory: 1073741824
```

**Observed:** ~1GB used out of 1GB ‚Üí hitting maxmemory

---

## Step 3 ‚Äî Eviction / slow commands

```bash
redis-cli info stats
```

```
evicted_keys: 1200
expired_keys: 1500
```

**Interpretation:** Keys being evicted ‚Üí app cache misses ‚Üí DB pressure

---

## Step 4 ‚Äî Containment

* Temporarily increase `maxmemory`:

```bash
redis-cli config set maxmemory 2gb
```

* Use LRU eviction policy:

```bash
redis-cli config set maxmemory-policy allkeys-lru
```

‚úÖ Cache stabilizes, DB pressure drops

---

## Next steps if worsens

1. Add Redis clustering / sharding
2. Enable monitoring + alerts on `used_memory`
3. Profile app caching pattern ‚Üí prevent cache storm

---

# üåê DNS OUTAGE SIMULATION

## Pager Alert

> App cannot resolve backend hostnames ‚Üí failing requests

---

## Step 1 ‚Äî Test DNS resolution

```bash
dig backend.service.local
```

```
;; connection timed out; no servers could be reached
```

‚úÖ DNS failure confirmed

---

## Step 2 ‚Äî Check local resolver

```bash
cat /etc/resolv.conf
```

```
nameserver 10.0.0.2
```

```bash
ping 10.0.0.2
```

‚úÖ Resolver unreachable

---

## Step 3 ‚Äî Containment

* Restart local DNS service / kube-dns:

```bash
systemctl restart systemd-resolved
kubectl rollout restart deployment coredns -n kube-system
```

‚úÖ Resolution restored, app connectivity returns

---

## If this worsens

1. Switch to secondary resolver (`8.8.8.8`) temporarily
2. Use static `/etc/hosts` overrides for critical services
3. Investigate upstream DNS provider

---

# üéØ LIVE WHITEBOARD INTERVIEW ‚Äî ‚ÄúDESIGN A RESILIENT SYSTEM‚Äù

**Prompt:** Design a resilient multi-tier web system.

**Model Answer / Thought Process:**

1. **Frontend layer**

   * Load balancer (HAProxy / Nginx / ALB)
   * Auto-scaling web servers
   * Health checks + rate limiting

2. **Application layer**

   * Stateless containers or VMs
   * Horizontal scaling
   * Graceful shutdown + rolling updates

3. **Database layer**

   * Primary + read replicas
   * Connection pooling
   * Backups + failover

4. **Caching layer**

   * Redis / Memcached cluster
   * LRU eviction, persistent failover

5. **Observability**

   * Metrics (Prometheus, Grafana)
   * Logging + alerting
   * Distributed tracing (Jaeger / OpenTelemetry)

6. **Failure scenarios considered**

   * Node / pod crash ‚Üí HPA / replication
   * Disk full ‚Üí monitoring + retention policies
   * Network partition ‚Üí retries, circuit breakers
   * DNS / service discovery ‚Üí multiple resolvers

7. **Automation / IaC**

   * Terraform / Helm / Ansible to version configs
   * CI/CD pipelines with canary rollout
   * Runbooks for common failures

‚úÖ Key: explain reasoning **step by step**, not just draw boxes

---

